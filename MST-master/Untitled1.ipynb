{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--content CONTENT]\n",
      "                             [--content_dir CONTENT_DIR] [--style STYLE]\n",
      "                             [--style_dir STYLE_DIR]\n",
      "                             [--vgg_weights VGG_WEIGHTS]\n",
      "                             [--decoder_weights DECODER_WEIGHTS]\n",
      "                             [--content_size CONTENT_SIZE]\n",
      "                             [--style_size STYLE_SIZE] [--crop]\n",
      "                             [--save_ext SAVE_EXT] [--gpu GPU]\n",
      "                             [--output_dir OUTPUT_DIR] [--alpha ALPHA]\n",
      "                             [--patch_size PATCH_SIZE]\n",
      "                             [--n_clusters_s N_CLUSTERS_S]\n",
      "                             [--graphPara_smooth GRAPHPARA_SMOOTH]\n",
      "                             [--graphPara_max_cycles GRAPHPARA_MAX_CYCLES]\n",
      "                             [--data_format DATA_FORMAT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/apple/Library/Jupyter/runtime/kernel-b5f2cc3d-8254-49dc-9845-29d65ae9b333.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from itertools import product\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import logging\n",
    "import maxflow\n",
    "from maxflow import fastmin\n",
    "\n",
    "from src.image import load_image, prepare_image, save_image\n",
    "from src.nn import build_vgg, build_decoder\n",
    "from src.norm import wct\n",
    "from src.weights import open_weights\n",
    "from src.util import get_filename, get_params, extract_image_names_recursive\n",
    "\n",
    "def style_transfer(\n",
    "        content=None,\n",
    "        content_dir=\"/Users/apple/Downloads/MST-master/data/content/sailboat.jpg\",\n",
    "        content_size=512,\n",
    "        style=None,\n",
    "        style_dir=\"/Users/apple/Downloads/MST-master/data/style/09.jpg\",\n",
    "        style_size=512,\n",
    "        crop=None,\n",
    "        alpha=1.0,\n",
    "        output_dir='output',\n",
    "        save_ext='jpg',\n",
    "        gpu=0,\n",
    "        vgg_weights='models/vgg19_weights_normalized.h5',\n",
    "        decoder_weights='models/ckp-MST-paper',\n",
    "        patch_size=3,\n",
    "        n_clusters_s=3,\n",
    "        graphPara_smooth=0.1,\n",
    "        graphPara_max_cycles=3,\n",
    "        data_format = 'channels_first'):\n",
    "    assert bool(content) != bool(content_dir), 'Either content or content_dir should be given'\n",
    "    assert bool(style) != bool(style_dir), 'Either style or style_dir should be given'\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        print('Creating output dir at', output_dir)\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Assume that it is either an h5 file or a name of a TensorFlow checkpoint\n",
    "    decoder_in_h5 = decoder_weights.endswith('.h5')\n",
    "\n",
    "    if content:\n",
    "        content_batch = [content]\n",
    "    else:\n",
    "        content_batch = extract_image_names_recursive(content_dir)\n",
    "\n",
    "    if style:\n",
    "        style_batch = [style]\n",
    "    else:\n",
    "        style_batch = extract_image_names_recursive(style_dir)\n",
    "\n",
    "    print('Number of content images:', len(content_batch))\n",
    "    print('Number of style images:', len(style_batch))\n",
    "    total_output_batch = len(content_batch) * len(style_batch)\n",
    "    print('Total number of output:', total_output_batch)\n",
    "\n",
    "    image, content, style, target, encoder, decoder = _build_graph(vgg_weights,\n",
    "        decoder_weights if decoder_in_h5 else None, alpha, patch_size, data_format=data_format)\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        if decoder_in_h5:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, decoder_weights)\n",
    "\n",
    "        for content_path, style_path in product(content_batch, style_batch):\n",
    "            content_name = get_filename(content_path)\n",
    "            content_image = load_image(content_path, content_size, crop)\n",
    "\n",
    "            style_name = get_filename(style_path)\n",
    "            style_image = load_image(style_path, style_size, crop)\n",
    "\n",
    "            style_image = prepare_image(style_image)\n",
    "            content_image = prepare_image(content_image)\n",
    "            style_feature = sess.run(encoder, feed_dict={\n",
    "                image: style_image[np.newaxis,:]\n",
    "            })\n",
    "            content_feature = sess.run(encoder, feed_dict={\n",
    "                image: content_image[np.newaxis,:]\n",
    "            })\n",
    "\n",
    "            # style_feature and content_feature information\n",
    "            Bc,Cc,Hc,Wc = content_feature.shape\n",
    "            Bs,Cs,Hs,Ws = style_feature.shape\n",
    "            c_feat_rec_HWxC = np.zeros((Hc*Wc, Cc))\n",
    "\n",
    "            # reshape content feature\n",
    "            c_feat_HWxC = BxCxHxW_to_HWxC(content_feature)\n",
    "\n",
    "            # cluster style feature\n",
    "            s_feat_HWxC = BxCxHxW_to_HWxC(style_feature)\n",
    "            s_cluster_centers, s_cluster_labels = cluster_feature(s_feat_HWxC, n_clusters_s)\n",
    "\n",
    "            # construct D\n",
    "            graphPara_D = np.double(1 - cosine_similarity(c_feat_HWxC, s_cluster_centers))\n",
    "            # construct V\n",
    "            X, Y = np.mgrid[:n_clusters_s, :n_clusters_s]\n",
    "            graphPara_V = graphPara_smooth*np.float_(np.abs(X-Y))\n",
    "            # graph cut\n",
    "            graphPara_sol = fastmin.aexpansion_grid(graphPara_D, graphPara_V, graphPara_max_cycles)\n",
    "            # ST \n",
    "            for label_idx in range(n_clusters_s):\n",
    "                print(\"#%d cluster:\" % label_idx)\n",
    "                # select content feature\n",
    "                c_subset_index = np.argwhere(graphPara_sol == label_idx).flatten()\n",
    "                c_subset_sample = c_feat_HWxC[c_subset_index,:]\n",
    "                c_subset_sample = HWxC_to_BxCxHWxW0(c_subset_sample)\n",
    "                print(\"c_subset_sample:\", c_subset_sample.shape)\n",
    "                # select cooresponding style feature\n",
    "                s_subset_index = np.argwhere(s_cluster_labels == label_idx).flatten()\n",
    "                s_subset_sample = s_feat_HWxC[s_subset_index,:]\n",
    "                s_subset_sample = HWxC_to_BxCxHWxW0(s_subset_sample)\n",
    "                print(\"s_subset_sample:\", s_subset_sample.shape)\n",
    "                # feature transfer\n",
    "                t_subset_sample = sess.run(target, feed_dict={\n",
    "                    content: c_subset_sample,\n",
    "                    style: s_subset_sample\n",
    "                })\n",
    "                \n",
    "                # target feature subset\n",
    "                t_subset_sample = BxCxHxW_to_HWxC(t_subset_sample)\n",
    "                c_feat_rec_HWxC[c_subset_index,:] = t_subset_sample\n",
    "            # reshape to target feature\n",
    "            target_feature = HWxC_to_BxCxHxW(c_feat_rec_HWxC, Hc, Wc, Cc)\n",
    "                 \n",
    "            # obtain output\n",
    "            output = sess.run(decoder, feed_dict={\n",
    "                content: content_feature,\n",
    "                target: target_feature\n",
    "            })\n",
    "\n",
    "            filename = '%s_stylized_%s.%s' % (content_name, style_name, save_ext)\n",
    "            filename = os.path.join(output_dir, filename)\n",
    "            save_image(filename, output[0], data_format=data_format)\n",
    "            print('Output image saved at', filename)\n",
    "        end_time = time.time()\n",
    "        print('Total outputs=' + str(total_output_batch) + ', total time=' + str(end_time - start_time) + ', average time=' + str((end_time-start_time)/total_output_batch))\n",
    "\n",
    "def _build_graph(vgg_weights, decoder_weights, alpha, patch_size, data_format):\n",
    "    if data_format == 'channels_first':\n",
    "        image = tf.placeholder(shape=(None,3,None,None), dtype=tf.float32)\n",
    "        content = tf.placeholder(shape=(1,512,None,None), dtype=tf.float32)\n",
    "        style = tf.placeholder(shape=(1,512,None,None), dtype=tf.float32)\n",
    "    else:\n",
    "        image = tf.placeholder(shape=(None,None,None,3), dtype=tf.float32)\n",
    "        content = tf.placeholder(shape=(1,None,None,512), dtype=tf.float32)\n",
    "        style = tf.placeholder(shape=(1,None,None,512), dtype=tf.float32)\n",
    "\n",
    "    target = wct(content, style, num_feature=512)\n",
    "\n",
    "    weighted_target = target * alpha + (1 - alpha) * content\n",
    "\n",
    "    with open_weights(vgg_weights) as w:\n",
    "        vgg = build_vgg(image, w, data_format=data_format)\n",
    "        encoder = vgg['conv4_1']\n",
    "\n",
    "    if decoder_weights:\n",
    "        with open_weights(decoder_weights) as w:\n",
    "            decoder = build_decoder(weighted_target, w, trainable=False,\n",
    "                data_format=data_format)\n",
    "    else:\n",
    "        decoder = build_decoder(weighted_target, None, trainable=False,\n",
    "            data_format=data_format)\n",
    "\n",
    "    return image, content, style, target, encoder, decoder\n",
    "\n",
    "def BxCxHxW_to_HWxC(feature_BNHW):\n",
    "    # squeeze: BxNxHxW -> NxHxW\n",
    "    feature = np.squeeze(feature_BNHW, axis=0)\n",
    "    # reshape: NxHxW -> NxHW\n",
    "    C, H, W = feature.shape\n",
    "    feature = np.reshape(feature, (C, H*W))\n",
    "    # transpose: NxHW -> HWxN\n",
    "    feature = np.transpose(feature)\n",
    "    return feature\n",
    "\n",
    "def HWxC_to_BxCxHxW(feature_HWxC, H, W, C):\n",
    "    # transpose: HWxC -> CxHW\n",
    "    feature = np.transpose(feature_HWxC)\n",
    "    # reshape: CxHW -> CxHxW\n",
    "    feature = np.reshape(feature, (C,H,W))\n",
    "    # expand_dim: CxHxW -> BxCxHxW\n",
    "    feature = np.expand_dims(feature, axis=0)\n",
    "    return feature\n",
    "\n",
    "def HWxC_to_BxCxHWxW0(feature_HWxC):\n",
    "    # HWxC -> CxHW\n",
    "    feature = np.transpose(feature_HWxC)\n",
    "    # CxHW -> BxCxHW\n",
    "    feature = np.expand_dims(feature, axis=0)\n",
    "    # BxCxHW -> BxCxHWxW0\n",
    "    feature = np.expand_dims(feature, axis=3)\n",
    "    return feature\n",
    "\n",
    "def cluster_feature(feature_HWxN, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(feature_HWxN)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_labels  = kmeans.labels_\n",
    "    return cluster_centers, cluster_labels\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    params = get_params(style_transfer)\n",
    "    parser = argparse.ArgumentParser(description='Multimodal Style Transfer via Graph Cuts')\n",
    "\n",
    "    parser.add_argument('--content', default=params['content'], help='File path to the content image')\n",
    "    parser.add_argument('--content_dir', default=params['content_dir'], help=\"\"\"Directory path to a batch of\n",
    "        content images\"\"\")\n",
    "    parser.add_argument('--style', default=params['style'], help=\"\"\"File path to the style image,\n",
    "        or multiple style images separated by commas if you want to do style\n",
    "        interpolation or spatial control\"\"\")\n",
    "    parser.add_argument('--style_dir', default=params['style_dir'],  help=\"\"\"Directory path to a batch of\n",
    "        style images\"\"\")\n",
    "    parser.add_argument('--vgg_weights', default=params['vgg_weights'],\n",
    "        help='Path to the weights of the VGG19 network')\n",
    "    parser.add_argument('--decoder_weights', default=params['decoder_weights'],\n",
    "        help='Path to the decoder')\n",
    "    parser.add_argument('--content_size', default=params['content_size'],\n",
    "        type=int, help=\"\"\"Maximum size for the content image, keeping\n",
    "        the original size if set to 0\"\"\")\n",
    "    parser.add_argument('--style_size', default=params['style_size'], type=int,\n",
    "        help=\"\"\"Maximum size for the style image, keeping the original\n",
    "        size if set to 0\"\"\")\n",
    "    parser.add_argument('--crop', action='store_true', help=\"\"\"If set, center\n",
    "        crop both content and style image before processing\"\"\")\n",
    "    parser.add_argument('--save_ext', default=params['save_ext'],\n",
    "        help='The extension name of the output image')\n",
    "    parser.add_argument('--gpu', default=params['gpu'], type=int,\n",
    "        help='Zero-indexed ID of the GPU to use; for CPU mode set to -1')\n",
    "    parser.add_argument('--output_dir', default=params['output_dir'],\n",
    "        help='Directory to save the output image(s)')\n",
    "    parser.add_argument('--alpha', default=params['alpha'], type=float,\n",
    "        help=\"\"\"The weight that controls the degree of stylization. Should be\n",
    "        between 0 and 1\"\"\")\n",
    "    parser.add_argument('--patch_size', default=params['patch_size'], type=int,\n",
    "        help=\"\"\"Patch size for patch matching\"\"\")\n",
    "    parser.add_argument('--n_clusters_s', default=params['n_clusters_s'], type=int,\n",
    "        help=\"\"\"number of cluster center of style\"\"\")\n",
    "    parser.add_argument('--graphPara_smooth', default=params['graphPara_smooth'], type=float,\n",
    "        help=\"\"\"smooth factor in graph cut\"\"\")\n",
    "    parser.add_argument('--graphPara_max_cycles', default=params['graphPara_max_cycles'], type=int,\n",
    "        help=\"\"\"cycle factor in graph cut\"\"\")\n",
    "    parser.add_argument('--data_format', default=params['data_format'],\n",
    "        help='data_format: channels_first or channels_last')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    style_transfer(**vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
